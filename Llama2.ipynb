{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu9iwXW1bGaV",
        "outputId": "058d30cc-d2de-4431-a3e4-c7ff6c0cb1a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing path to project directory and installing all requirements\n",
        "%cd /content/drive/MyDrive/\"Colab Notebooks\"/LlamaCodeFromScratch/\n",
        "#!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "c-PN0qnn5-RC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5971773-70a2-48d6-e5fc-6f6d4b3052b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/LlamaCodeFromScratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 4096\n",
        "    n_layers: int = 32\n",
        "    n_heads: int = 32\n",
        "    n_kv_heads: Optional[int] = None\n",
        "    vocab_size: int = -1 # Later set in the build method\n",
        "    multiple_of: int = 256\n",
        "    ffn_dim_multiplier: Optional[float] = None\n",
        "    norm_eps: float = 1e-5\n",
        "\n",
        "    # Needed for KV cache\n",
        "    max_batch_size: int = 32\n",
        "    max_seq_len: int = 2048\n",
        "\n",
        "    device: str = None\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # The gamma parameter\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x: torch.Tensor):\n",
        "        # (B, Seq_Len, Dim) * (B, Seq_Len, 1) = (B, Seq_Len, Dim)\n",
        "        # rsqrt: 1 / sqrt(x)\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # (Dim) * (B, Seq_Len, Dim) = (B, Seq_Len, Dim)\n",
        "        return self.weight * self._norm(x.float()).type_as(x)\n",
        "\n",
        "\n",
        "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0):\n",
        "    # As written in the paragraph 3.2.2 of the paper\n",
        "    # >> In order to generalize our results in 2D to any xi ∈ Rd where **d is even**, [...]\n",
        "    assert head_dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
        "    # Build the theta parameter\n",
        "    # According to the formula theta_i = 10000^(-2(i-1)/dim) for i = [1, 2, ... dim/2]\n",
        "    # Shape: (Head_Dim / 2)\n",
        "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
        "    # Shape: (Head_Dim / 2)\n",
        "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device) # (Dim / 2)\n",
        "    # Construct the positions (the \"m\" parameter)\n",
        "    # Shape: (Seq_Len)\n",
        "    m = torch.arange(seq_len, device=device)\n",
        "    # Multiply each theta by each position using the outer product.\n",
        "    # Shape: (Seq_Len) outer_product* (Head_Dim / 2) -> (Seq_Len, Head_Dim / 2)\n",
        "    freqs = torch.outer(m, theta).float()\n",
        "    # We can compute complex numbers in the polar form c = R * exp(m * theta), where R = 1 as follows:\n",
        "    # (Seq_Len, Head_Dim / 2) -> (Seq_Len, Head_Dim / 2)\n",
        "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
        "    return freqs_complex\n",
        "\n",
        "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
        "    # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
        "    # Two consecutive values will become a single complex number\n",
        "    # (B, Seq_Len, H, Head_Dim) -> (B, Seq_Len, H, Head_Dim/2)\n",
        "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "    # Reshape the freqs_complex tensor to match the shape of the x_complex tensor. So we need to add the batch dimension and the head dimension\n",
        "    # (Seq_Len, Head_Dim/2) --> (1, Seq_Len, 1, Head_Dim/2)\n",
        "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "    # Multiply each complex number in the x_complex tensor by the corresponding complex number in the freqs_complex tensor\n",
        "    # Which results in the rotation of the complex number as shown in the Figure 1 of the paper\n",
        "    # (B, Seq_Len, H, Head_Dim/2) * (1, Seq_Len, 1, Head_Dim/2) = (B, Seq_Len, H, Head_Dim/2)\n",
        "    x_rotated = x_complex * freqs_complex\n",
        "    # Convert the complex number back to the real number\n",
        "    # (B, Seq_Len, H, Head_Dim/2) -> (B, Seq_Len, H, Head_Dim/2, 2)\n",
        "    x_out = torch.view_as_real(x_rotated)\n",
        "    # (B, Seq_Len, H, Head_Dim/2, 2) -> (B, Seq_Len, H, Head_Dim)\n",
        "    x_out = x_out.reshape(*x.shape)\n",
        "    return x_out.type_as(x).to(device)\n",
        "\n",
        "\n",
        "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "        return x\n",
        "    return (\n",
        "        # (B, Seq_Len, N_KV_Heads, 1, Head_Dim)\n",
        "        x[:, :, :, None, :]\n",
        "        # (B, Seq_Len, N_KV_Heads, N_Rep, Head_Dim)\n",
        "        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
        "        # (B, Seq_Len, N_KV_Heads * N_Rep, Head_Dim)\n",
        "        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
        "    )\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "\n",
        "        # Indicates the number of heads for the Keys and Values\n",
        "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "        # Indicates the number of heads for the Queries\n",
        "        self.n_heads_q = args.n_heads\n",
        "        # Indicates how many times the Keys and Values should be repeated\n",
        "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
        "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "\n",
        "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        start_pos: int,\n",
        "        freqs_complex: torch.Tensor\n",
        "    ):\n",
        "        batch_size, seq_len, _ = x.shape  # (B, 1, Dim)\n",
        "\n",
        "        # (B, 1, Dim) -> (B, 1, H_Q * Head_Dim)\n",
        "        xq = self.wq(x)\n",
        "        # (B, 1, Dim) -> (B, 1, H_KV * Head_Dim)\n",
        "        xk = self.wk(x)\n",
        "        # (B, 1, Dim) -> (B, 1, H_KV * Head_Dim)\n",
        "        xv = self.wv(x)\n",
        "\n",
        "        # (B, 1, H_Q * Head_Dim) -> (B, 1, H_Q, Head_Dim)\n",
        "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
        "        # (B, 1, H_KV * Head_Dim) -> (B, 1, H_KV, Head_Dim)\n",
        "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "        # (B, 1, H_KV * Head_Dim) -> (B, 1, H_KV, Head_Dim)\n",
        "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "        # (B, 1, H_Q, Head_Dim) --> (B, 1, H_Q, Head_Dim)\n",
        "        xq = apply_rotary_embeddings(xq, freqs_complex, device=x.device)\n",
        "        # (B, 1, H_KV, Head_Dim) --> (B, 1, H_KV, Head_Dim)\n",
        "        xk = apply_rotary_embeddings(xk, freqs_complex, device=x.device)\n",
        "\n",
        "        # Replace the entry in the cache\n",
        "        self.cache_k[:batch_size, start_pos : start_pos + seq_len] = xk\n",
        "        self.cache_v[:batch_size, start_pos : start_pos + seq_len] = xv\n",
        "\n",
        "        # (B, Seq_Len_KV, H_KV, Head_Dim)\n",
        "        keys = self.cache_k[:batch_size, : start_pos + seq_len]\n",
        "        # (B, Seq_Len_KV, H_KV, Head_Dim)\n",
        "        values = self.cache_v[:batch_size, : start_pos + seq_len]\n",
        "\n",
        "        # Since every group of Q shares the same K and V heads, just repeat the K and V heads for every Q in the same group.\n",
        "\n",
        "        # (B, Seq_Len_KV, H_KV, Head_Dim) --> (B, Seq_Len_KV, H_Q, Head_Dim)\n",
        "        keys = repeat_kv(keys, self.n_rep)\n",
        "        # (B, Seq_Len_KV, H_KV, Head_Dim) --> (B, Seq_Len_KV, H_Q, Head_Dim)\n",
        "        values = repeat_kv(values, self.n_rep)\n",
        "\n",
        "        # (B, 1, H_Q, Head_Dim) -> (B, H_Q, 1, Head_Dim)\n",
        "        xq = xq.transpose(1, 2)\n",
        "        # (B, Seq_Len_KV, H_Q, Head_Dim) -> (B, H_Q, Seq_Len_KV, Head_Dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        # (B, Seq_Len_KV, H_Q, Head_Dim) -> (B, H_Q, Seq_Len_KV, Head_Dim)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # (B, H_Q, 1, Head_Dim) @ (B, H_Q, Head_Dim, Seq_Len_KV) -> (B, H_Q, 1, Seq_Len_KV)\n",
        "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        # (B, H_Q, 1, Seq_Len_KV) -> (B, H_Q, 1, Seq_Len_KV)\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "\n",
        "        # (B, H_Q, 1, Seq_Len) @ (B, H_Q, Seq_Len_KV, Head_Dim) -> (B, H_Q, 1, Head_Dim)\n",
        "        output = torch.matmul(scores, values)\n",
        "        # (B, H_Q, 1, Head_Dim) -> (B, 1, H_Q, Head_Dim) -> (B, 1, Dim)\n",
        "        output = (output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1))\n",
        "        return self.wo(output) # (B, 1, Dim) -> (B, 1, Dim)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        args: ModelArgs\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_dim = 4 * args.dim\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        if args.ffn_dim_multiplier is not None:\n",
        "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
        "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
        "        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
        "\n",
        "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
        "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
        "        swish = F.silu(self.w1(x))\n",
        "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
        "        x_V = self.w3(x)\n",
        "        # (B, Seq_Len, Hidden_Dim) * (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Hidden_Dim)\n",
        "        x = swish * x_V\n",
        "        # (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Dim)\n",
        "        x = self.w2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "        self.attention = SelfAttention(args)\n",
        "        self.feed_forward = FeedForward(args)\n",
        "\n",
        "        # Normalization BEFORE the attention block\n",
        "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        # Normalization BEFORE the feed forward block\n",
        "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "        # (B, Seq_Len, Dim) + (B, Seq_Len, Dim) --> (B, Seq_Len, Dim)\n",
        "        h = x + self.attention.forward(\n",
        "            self.attention_norm(x), start_pos, freqs_complex\n",
        "        )\n",
        "        # (B, Seq_Len, Dim) + (B, Seq_Len, Dim) --> (B, Seq_Len, Dim)\n",
        "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "\n",
        "        assert args.vocab_size != -1, \"Vocab size must be set\"\n",
        "\n",
        "        self.args = args\n",
        "        self.vocab_size = args.vocab_size\n",
        "        self.n_layers = args.n_layers\n",
        "        self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for layer_id in range(args.n_layers):\n",
        "            self.layers.append(EncoderBlock(args))\n",
        "\n",
        "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
        "\n",
        "        self.freqs_complex = precompute_theta_pos_frequencies(self.args.dim // self.args.n_heads, self.args.max_seq_len * 2, device=self.args.device)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
        "        # (B, Seq_Len)\n",
        "\n",
        "        # (B, Seq_Len) -> (B, Seq_Len, Dim)\n",
        "        h = self.tok_embeddings(tokens)\n",
        "\n",
        "        # Retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
        "        freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
        "\n",
        "        # Consecutively apply all the encoder layers\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, start_pos, freqs_complex)\n",
        "        h = self.norm(h)\n",
        "        output = self.output(h).float()\n",
        "        return output"
      ],
      "metadata": {
        "id": "h0me8Zd4b8xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "import time\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class LLaMA:\n",
        "\n",
        "    def __init__(self, model: Transformer, tokenizer: SentencePieceProcessor, model_args: ModelArgs):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.args = model_args\n",
        "\n",
        "    @staticmethod\n",
        "    def build(checkpoints_dir: str, tokenizer_path: str, load_model: bool, max_seq_len: int, max_batch_size: int, device: str):\n",
        "        prev_time = time.time()\n",
        "        if load_model:\n",
        "            checkpoints = sorted(Path(checkpoints_dir).glob(\"*.pth\"))\n",
        "            assert len(checkpoints) > 0, f\"no checkpoint files found in {checkpoints_dir}\"\n",
        "            ckpt_path = checkpoints[0]\n",
        "            print(f'Loading checkpoint \"{ckpt_path}\"')\n",
        "            checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "            print(f\"Loaded checkpoint in {time.time() - prev_time:.2f}s\")\n",
        "            prev_time = time.time()\n",
        "        with open(Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
        "            params = json.loads(f.read())\n",
        "\n",
        "        model_args: ModelArgs = ModelArgs(\n",
        "            max_seq_len=max_seq_len,\n",
        "            max_batch_size=max_batch_size,\n",
        "            device=device,\n",
        "            **params\n",
        "        )\n",
        "\n",
        "        tokenizer = SentencePieceProcessor()\n",
        "        tokenizer.load(tokenizer_path)\n",
        "        model_args.vocab_size = tokenizer.vocab_size()\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "        else:\n",
        "            torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
        "\n",
        "        model = Transformer(model_args).to(device)\n",
        "\n",
        "        if load_model:\n",
        "            # The only unmatched key in the checkpoint is rope.freqs. Remove it\n",
        "            del checkpoint['rope.freqs']\n",
        "            model.load_state_dict(checkpoint, strict=True)\n",
        "            print(f\"Loaded state dict in {time.time() - prev_time:.2f}s\")\n",
        "\n",
        "        return LLaMA(model, tokenizer, model_args)\n",
        "\n",
        "    def text_completion(self, prompts: list[str], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None):\n",
        "        if max_gen_len is None:\n",
        "            max_gen_len = self.args.max_seq_len - 1\n",
        "        # Convert each prompt into tokens\n",
        "        prompt_tokens = [self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False) for prompt in prompts]\n",
        "        # Make sure the batch size is not too large\n",
        "        batch_size = len(prompt_tokens)\n",
        "        assert batch_size <= self.args.max_batch_size, f\"batch size must be less than or equal to {self.args.max_batch_size}\"\n",
        "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
        "        # Make sure the prompt length is not larger than the maximum sequence length\n",
        "        assert max_prompt_len <= self.args.max_seq_len, f\"prompt length must be less than or equal to {self.args.max_seq_len}\"\n",
        "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
        "\n",
        "        # Create the list that will contain the generated tokens, along with the initial prompt tokens\n",
        "        pad_id = self.tokenizer.pad_id()\n",
        "        tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=device)\n",
        "        for k, t in enumerate(prompt_tokens):\n",
        "            # Populate the initial tokens with the prompt tokens\n",
        "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
        "\n",
        "        eos_reached = torch.tensor([False] * batch_size, device=device)\n",
        "        prompt_tokens_mask = tokens != pad_id # True if the token is a prompt token, False otherwise\n",
        "        cur_iterator = tqdm(range(1, total_len), desc=\"Generating tokens\")\n",
        "        for cur_pos in cur_iterator:\n",
        "            with torch.no_grad():\n",
        "                logits = self.model.forward(tokens[:, cur_pos-1:cur_pos], cur_pos)\n",
        "            if temperature > 0:\n",
        "                # The temperature is applied before the softmax\n",
        "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
        "                next_token = self._sample_top_p(probs, top_p)\n",
        "            else:\n",
        "                # Greedily select the token with the max probability\n",
        "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
        "\n",
        "            next_token = next_token.reshape(-1)\n",
        "            # Only replace token if it is a padding token\n",
        "            next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
        "            tokens[:, cur_pos] = next_token\n",
        "            # EOS is reached only if we found an EOS token for a padding position\n",
        "            eos_reached |= (~prompt_tokens_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id)\n",
        "            if all(eos_reached):\n",
        "                break\n",
        "\n",
        "        out_tokens = []\n",
        "        out_text = []\n",
        "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
        "            # Cut to the EOS token, if present\n",
        "            if self.tokenizer.eos_id in current_prompt_tokens:\n",
        "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
        "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
        "            out_tokens.append(current_prompt_tokens)\n",
        "            out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
        "        return (out_tokens, out_text)\n",
        "\n",
        "    def _sample_top_p(self, probs, p):\n",
        "        # (B, vocab_size)\n",
        "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "        # (B, vocab_size)\n",
        "        probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "        # (B, vocab_size)\n",
        "        # (Substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
        "        mask = probs_sum - probs_sort > p\n",
        "        # Zero out all the probabilities of tokens that are not selected by the Top P\n",
        "        probs_sort[mask] = 0.0\n",
        "        # Redistribute the probabilities so that they sum up to 1.\n",
        "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "        # Sample a token (its index) from the top p distribution\n",
        "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "        # Get the token position in the vocabulary corresponding to the sampled index\n",
        "        next_token = torch.gather(probs_idx, -1, next_token)\n",
        "        return next_token\n"
      ],
      "metadata": {
        "id": "ubK8bOXocEX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "allow_cuda = False\n",
        "device = 'cuda' if torch.cuda.is_available() and allow_cuda else 'cpu'\n",
        "\n",
        "# prompts = [\n",
        "#     \"Simply put, the theory of relativity states that \",\n",
        "#     \"If Google was an Italian company founded in Milan, it would\",\n",
        "#     # Few shot promt\n",
        "#     \"\"\"Translate English to French:\n",
        "\n",
        "#     sea otter => loutre de mer\n",
        "#     peppermint => menthe poivrée\n",
        "#     plush girafe => girafe peluche\n",
        "#     cheese =>\"\"\",\n",
        "#     # Zero shot prompt\n",
        "#     \"\"\"Tell me if the following person is actually Doraemon disguised as human:\n",
        "#     Decision:\n",
        "#     \"\"\"\n",
        "# ]\n",
        "\n",
        "prompts = [\n",
        "    \"Write a summary for below sentence: Miss Brill' is the story of an old woman told brilliantly and realistically, balancing thoughts and emotions that sustain her late solitary life amidst all the bustle of modern life. Miss Brill is a regular visitor on Sundays to the Jardins Publiques (the Public Gardens) of a small French suburb where she sits and watches all sorts of people come and go. She listens to the band playing, loves to watch people and guess what keeps them going, and enjoys contemplating the world as a great stage upon which actors perform. She finds herself to be another actor among the so many she sees, or at least herself as 'part of the performance after all.' One Sunday Miss Brill puts on her fur and goes to the Public Gardens as usual. The evening ends with her sudden realization that she is old and lonely, a realization brought to her by a conversation she overhears between a boy and a girl, presumably lovers, who comment on her unwelcome presence in their vicinity. Miss Brill is sad and depressed as she returns home, not stopping by as usual to buy her Sunday delicacy, a slice of honey-cake. She retires to her dark room, puts the fur back into the box and imagines that she has heard something cry. \"\n",
        "]\n",
        "\n",
        "model = LLaMA.build(\n",
        "    checkpoints_dir='dataset/',\n",
        "    tokenizer_path='dataset/tokenizer.model',\n",
        "    load_model=True,\n",
        "    max_seq_len=1024,\n",
        "    max_batch_size=len(prompts),\n",
        "    device=device\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "act4mgAV36Ko",
        "outputId": "cbf8ca8f-d80a-45be-e800-985463832cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint \"dataset/consolidated.00.pth\"\n",
            "Loaded checkpoint in 136.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded state dict in 88.28s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "out_tokens, out_texts = (model.text_completion(prompts, max_gen_len=64))\n",
        "assert len(out_texts) == len(prompts)\n",
        "for i in range(len(out_texts)):\n",
        "    print(f'{out_texts[i]}')\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z30kuDXM4fue",
        "outputId": "2d1b6baf-1940-4d0a-be50-ca510a1dfb41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating tokens: 100%|██████████| 359/359 [36:32<00:00,  6.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a summary for below sentence: Miss Brill' is the story of an old woman told brilliantly and realistically, balancing thoughts and emotions that sustain her late solitary life amidst all the bustle of modern life. Miss Brill is a regular visitor on Sundays to the Jardins Publiques (the Public Gardens) of a small French suburb where she sits and watches all sorts of people come and go. She listens to the band playing, loves to watch people and guess what keeps them going, and enjoys contemplating the world as a great stage upon which actors perform. She finds herself to be another actor among the so many she sees, or at least herself as 'part of the performance after all.' One Sunday Miss Brill puts on her fur and goes to the Public Gardens as usual. The evening ends with her sudden realization that she is old and lonely, a realization brought to her by a conversation she overhears between a boy and a girl, presumably lovers, who comment on her unwelcome presence in their vicinity. Miss Brill is sad and depressed as she returns home, not stopping by as usual to buy her Sunday delicacy, a slice of honey-cake. She retires to her dark room, puts the fur back into the box and imagines that she has heard something cry. 1. What is the title of the story? 2. What is the theme of the story? 3. What is the central idea of the story? 4. Who is the protagonist of the story? 5. Who is the antagonist of the story? 6. What is the\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Tell steps to study python \"\n",
        "    ]\n",
        "\n",
        "\n",
        "out_tokens, out_texts = (model.text_completion(prompts, max_gen_len=256))\n",
        "assert len(out_texts) == len(prompts)\n",
        "for i in range(len(out_texts)):\n",
        "    print(f'{out_texts[i]}')\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR9WV3ykY6bJ",
        "outputId": "cedb9a1e-a4df-43f3-dc07-3baa7f6441c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating tokens: 100%|██████████| 262/262 [25:51<00:00,  5.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell steps to study python 3.…\n",
            "Tell steps to study python 3.x\n",
            "python - 3.x is the latest version of python. It is a very powerful and popular programming language. Python is a high-level, interpreted, and object-oriented programming language. Python is a versatile language that is used for a wide range of applications, including web development, data science, and machine learning. Python has a large and active community of developers, and there are many resources available for learning the language.\n",
            "The first step in learning python 3.x is to install the language on your computer. Python is available for free and can be downloaded from the Python website. Once you have installed Python, you will need to set up your environment. This involves installing the necessary libraries and tools for using Python.\n",
            "Once you have set up your environment, you can start learning the language. There are many resources available for learning python 3.x, including online tutorials, books, and courses. You can also find communities of python developers who can help you with your learning.\n",
            "Once you have learned the basics of python 3.x, you can start applying your knowledge to real-world projects. There are many open-source projects that you can contribute\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ynSXLzJ3gyik"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}